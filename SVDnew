import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import DataLoader, TensorDataset, Dataset
from torch.profiler import profile, ProfilerActivity

from models.SVD_3depth import SVDNet3LayerD
from models.SVD_3layer import SVDNet3Layer, SVDNetDeep
from models.SVD_prune import SVDNetPrune

# --- Constants ---
BATCH_SIZE = 64
TEST_RATIO = 0.2
NUM_EPOCHS = 500
LEARNING_RATE = 5e-4
#WEIGHT_DECAY = 1e-3
M, N, Q, r = 64, 64, 2, 32
EFFECTIVE_RANK = 8


# --- Utility Functions ---

def split_data_numpy(data, labels, test_ratio=0.2, seed=42):
    """
    Splits numpy arrays into training and testing sets without sklearn.
    """
    np.random.seed(seed)

    num_samples = data.shape[0]
    indices = np.arange(num_samples)
    np.random.shuffle(indices)

    split_idx = int(num_samples * (1 - test_ratio))

    train_indices = indices[:split_idx]
    test_indices = indices[split_idx:]

    train_data, test_data = data[train_indices], data[test_indices]
    train_labels, test_labels = labels[train_indices], labels[test_indices]

    return train_data, test_data, train_labels, test_labels


@torch.jit.script
def complex_matmul_fused(A, B):
    """
    A: (batch, M, K, 2)   where A[...,0]=real, A[...,1]=imag
    B: (batch, K, N, 2)
    returns: (batch, M, N, 2)
    """
    A_r, A_i = A[..., 0], A[..., 1]
    B_r, B_i = B[..., 0], B[..., 1]

    top_A = torch.cat([A_r, -A_i], dim=2)
    bot_A = torch.cat([A_i, A_r], dim=2)
    Ar = torch.cat([top_A, bot_A], dim=1)

    top_B = torch.cat([B_r, -B_i], dim=2)
    bot_B = torch.cat([B_i, B_r], dim=2)
    Br = torch.cat([top_B, bot_B], dim=1)

    C = torch.bmm(Ar, Br)

    M_shape = A.shape[1]
    N_shape = B.shape[2]
    real = C[:, :M_shape, :N_shape]
    imag = C[:, M_shape:, :N_shape]

    return torch.stack([real, imag], dim=-1)


def complex_hermitian(A):
    return torch.stack([A[..., 0].transpose(-2, -1), -A[..., 1].transpose(-2, -1)], dim=-1)


def get_avg_flops(model: nn.Module, input_data: torch.Tensor) -> float:
    """
    Estimates the average FLOPs per sample for a model using PyTorch Profiler.
    """
    if input_data.dim() == 0 or input_data.size(0) == 0:
        raise RuntimeError("Input data must have a non-zero batch dimension")

    batch_size = input_data.size(0)
    model = model.eval().cpu()
    input_data = input_data.cpu()

    with torch.no_grad():
        with profile(
                activities=[ProfilerActivity.CPU],
                with_flops=True,
                record_shapes=False
        ) as prof:
            model(input_data)

    total_flops = sum(event.flops for event in prof.events())
    avg_flops = total_flops / batch_size

    # Divide by 2 for MACs, and 1e6 for Mega
    return avg_flops * 1e-6 / 2


def load_combined_data():
    """Loads and combines training data from multiple files."""
    train_list = []
    label_list = []
    for i in range(1, 4):
        train_np = np.load(f'./CompetitionData1/Round1TrainData{i}.npy')
        label_np = np.load(f'./CompetitionData1/Round1TrainLabel{i}.npy')
        train_list.append(train_np)
        label_list.append(label_np)

    combined_train = np.concatenate(train_list, axis=0)
    combined_label = np.concatenate(label_list, axis=0)
    return combined_train, combined_label


# --- Dataset & Loss Classes ---

class NormalizedChannelDataset(Dataset):
    def __init__(self, data, labels, mean, std):
        self.data = data
        self.labels = labels
        self.mean = mean
        self.std = std

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x = self.data[idx]
        y = self.labels[idx]

        sample_mean = np.mean(x)
        sample_std = np.std(x)
        #x_normalized = (x - self.mean) / (self.std + 1e-8)
        x_normalized = (x - sample_mean) / (sample_std + 1e-8)

        return torch.from_numpy(x_normalized).float(), torch.from_numpy(y).float()


class ApproximationErrorLoss(nn.Module):
    def __init__(self):
        super(ApproximationErrorLoss, self).__init__()

    def forward(self, U, s, V, H_label):
        # Dynamically get batch_size and r from the input tensor shape
        batch_size, _, r, _ = U.shape

        Sigma = torch.diag_embed(s)
        Sigma_complex = torch.stack([Sigma, torch.zeros_like(Sigma)], dim=-1)

        V_H = complex_hermitian(V)
        U_Sigma = complex_matmul_fused(U, Sigma_complex)
        H_pred = complex_matmul_fused(U_Sigma, V_H)

        h_flat = H_label.view(batch_size, -1)
        diff_flat = (H_label - H_pred).view(batch_size, -1)

        norm_h_label = torch.norm(h_flat, p=2, dim=1)
        norm_diff = torch.norm(diff_flat, p=2, dim=1)
        reconstruction_loss = (norm_diff / (norm_h_label + 1e-8)).mean()

        U_H = complex_hermitian(U)
        U_H_U = complex_matmul_fused(U_H, U)
        V_H_V = complex_matmul_fused(V_H, V)

        I_r = torch.eye(r, device=U.device).expand(batch_size, r, r)
        I_r_complex = torch.stack([I_r, torch.zeros_like(I_r)], dim=-1)

        ortho_loss_U = torch.norm((U_H_U - I_r_complex).view(batch_size, -1), p=2, dim=1).mean()
        ortho_loss_V = torch.norm((V_H_V - I_r_complex).view(batch_size, -1), p=2, dim=1).mean()

        return reconstruction_loss + ortho_loss_U + ortho_loss_V


# --- Core Logic ---

def train(model: nn.Module, model_path: str):
    """Trains the SVDNet model."""
    device = next(model.parameters()).device

    all_data_np, all_labels_np = load_combined_data()

    train_np, test_np, train_labels_np, test_labels_np = split_data_numpy(
        all_data_np, all_labels_np, test_ratio=TEST_RATIO, seed=42
    )

    # Calculate the percentile thresholds from the training data
    lower_bound = np.percentile(train_np, 1)
    upper_bound = np.percentile(train_np, 99)

    # Clip both the training and test data using these bounds
    train_np_clipped = np.clip(train_np, lower_bound, upper_bound)
    test_np_clipped = np.clip(test_np, lower_bound, upper_bound)

    # Then, proceed with your standard normalization on the clipped data
    mean = np.mean(train_np_clipped)
    std = np.std(train_np_clipped)

    train_dataset = NormalizedChannelDataset(train_np, train_labels_np, mean, std)
    test_dataset = NormalizedChannelDataset(test_np, test_labels_np, mean, std)

    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

    loss_fn = ApproximationErrorLoss().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS + 1)

    best_loss = float('inf')
    best_state_dict = None

    print("--- Starting Training ---")
    for epoch in range(NUM_EPOCHS):
        model.train()

        num_batches = len(train_dataloader)
        print(f"Epoch {epoch + 1}/{NUM_EPOCHS}")

        for i, (h_noise, h_label) in enumerate(train_dataloader):
            h_noise, h_label = h_noise.to(device), h_label.to(device)

            optimizer.zero_grad()
            U_pred, s_pred, V_pred = model(h_noise)

            s_pred_mod = s_pred.clone()
            s_pred_mod[:, EFFECTIVE_RANK:] = 0

            loss = loss_fn(U_pred, s_pred_mod, V_pred, h_label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            print(f"  Batch {i + 1}/{num_batches} processed...", end='\r')

        print(" " * 50, end='\r')

        model.eval()
        total_test_loss = 0.0
        with torch.no_grad():
            for h_noise, h_label in test_loader:
                h_noise, h_label = h_noise.to(device), h_label.to(device)
                U_pred, s_pred, V_pred = model(h_noise)

                s_pred_mod = s_pred.clone()
                s_pred_mod[:, EFFECTIVE_RANK:] = 0

                test_loss = loss_fn(U_pred, s_pred_mod, V_pred, h_label)
                total_test_loss += test_loss.item()

        mean_test_loss = total_test_loss / len(test_loader)
        print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}], Test Loss: {mean_test_loss:.4f}")

        if mean_test_loss < best_loss:
            best_loss = mean_test_loss
            best_state_dict = model.state_dict()

    scheduler.step()
    print(f"\n--- Training Finished ---\n Best Mean Test Loss: {best_loss:.4f}")
    model.load_state_dict(best_state_dict)

    #Save best model
    torch.save(best_state_dict, model_path)
    print(f"Model saved to {model_path}")


def test_model(model: nn.Module, test_data_path: str, file_name: str):
    """
    Evaluates the model, creating a rank-32 output where the singular
    values are padded to have an effective rank of 8.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    test_np = np.load(test_data_path)

    # --- Added normalization to test data for consistency with training ---
    sample_mean = np.mean(test_np)
    sample_std = np.std(test_np)
    test_np_normalized = (test_np - sample_mean) / (sample_std + 1e-8)
    test_tensor = torch.from_numpy(test_np_normalized).float()

    test_loader = DataLoader(TensorDataset(test_tensor), batch_size=BATCH_SIZE)


    U_list, s_list, V_list = [], [], []
    with torch.no_grad():
        for (h_test,) in test_loader:
            h_test = h_test.to(device)
            U, s, V = model(h_test)

            # --- Handle U and V: Truncate to the final desired rank of 32 ---
            U_final = U[:, :, :r, :]
            V_final = V[:, :, :r, :]

            # --- Handle s: Take top 8 values and pad with zeros to rank 32 ---
            s_top_k = s[:, :EFFECTIVE_RANK]

            padding_size = r - EFFECTIVE_RANK
            s_padding = torch.zeros(s.shape[0], padding_size, device=s.device)

            s_final = torch.cat([s_top_k, s_padding], dim=1)

            U_list.append(U_final.cpu())
            s_list.append(s_final.cpu())
            V_list.append(V_final.cpu())

    U_all, s_all, V_all = torch.cat(U_list), torch.cat(s_list), torch.cat(V_list)
    U_np, s_np, V_np = U_all.numpy(), s_all.numpy(), V_all.numpy()

    # Estimate model complexity using a sample from the test data
    mega_macs = get_avg_flops(model, test_tensor.to(device))
    print(f"Estimated MACs for {file_name}: {mega_macs:.2f} MegaMACs")

    np.savez(f"{file_name}.npz", U=U_np, S=s_np, V=V_np, C=mega_macs)
    print(f" Saved predictions to {file_name}.npz")


# --- Main Execution ---
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"--- Using device: {device} ---")
    model_path = "SVDNet.pth"
    model = SVDNetDeep(M=M, N=N, r=r).to(device)
    train(model, model_path)

    print("\n--- Starting Testing ---")
    for i in range(1, 4):
        test_data_path = f'./CompetitionData1/Round1TestData{i}.npy'
        submission_path = f"submission/{i}"
        test_model(model, test_data_path, submission_path)
